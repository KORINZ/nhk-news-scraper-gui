import datetime
import locale
import requests
import random
import string
import sys
import os

from bs4 import BeautifulSoup
from collections import deque
from line_message_bot import send_message
from selenium import webdriver
from selenium.webdriver.common.by import By
from typing import Dict, List, Tuple
from get_definition import get_definition_list

'''
NEWS WEB EASY
å¹³æ—¥ã®ã¿ã€4ã¤ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ›´æ–°ã•ã‚Œã‚‹
å‡ºæ‰€ã®æ˜ç¤ºã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å†…å®¹ã‚’è»¢è¼‰â—‹ èªå½™ã¯â—‹ è¾æ›¸â–³Ã—ï¼Ÿ

https://www.nhk.or.jp/nijiriyou/kyouiku.html
è‘—ä½œæ¨©æ³•ã§ã¯ã€å­¦æ ¡ãã®ä»–ã®æ•™è‚²æ©Ÿé–¢ã§ã€æˆæ¥­ã®æ•™æã¨ã—ã¦è¤‡è£½ã‚’èªã‚ã‚‹ä»£ã‚ã‚Šã«ã€ã€Œæˆæ¥­æ‹…å½“è€…ã¾ãŸã¯æˆæ¥­ã‚’å—ã‘ã‚‹è€…ãŒè¤‡è£½ã™ã‚‹ã“ã¨ã€ã‚„ã€Œå‡ºæ‰€ã®æ˜ç¤ºã€ãªã©ã€
ã„ãã¤ã‹ã®æ¡ä»¶ã‚’å®ˆã‚‹ã‚ˆã†æ±‚ã‚ã¦ã„ã¾ã™ã€‚
ã¾ãŸã€è‘—ä½œæ¨©æ³•ãŒä¸€éƒ¨æ”¹æ­£ã•ã‚Œã€ï¼’ï¼ï¼’ï¼å¹´ï¼”æœˆï¼’ï¼˜æ—¥ã‹ã‚‰ã€æ¨©åˆ©è€…ã®åˆ©ç›Šã‚’ä¸å½“ã«å®³ã—ãªã„ç¯„å›²ã§ã€å­¦æ ¡ã®å…ˆç”Ÿã‚„ç”Ÿå¾’ãŒã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆãªã©ã‚’é€šã˜ã¦ã€
ï¼®ï¼¨ï¼«ã®ç•ªçµ„ã‚’æ•™æã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚

https://sartras.or.jp/seido/
æˆæ¥­ç›®çš„å…¬è¡†é€ä¿¡è£œå„Ÿé‡‘åˆ¶åº¦
å…·ä½“çš„ã«ã¯ã€å­¦æ ¡ç­‰ã®æ•™è‚²æ©Ÿé–¢ã®æˆæ¥­ã§ã€äºˆç¿’ãƒ»å¾©ç¿’ç”¨ã«æ•™å“¡ãŒä»–äººã®è‘—ä½œç‰©ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ•™æã‚’ç”Ÿå¾’ã®ç«¯æœ«ã«é€ä¿¡ã—ãŸã‚Šã€
ã‚µãƒ¼ãƒã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚Šã™ã‚‹ã“ã¨ãªã©ã€ICTã®æ´»ç”¨ã«ã‚ˆã‚Šæˆæ¥­ã®éç¨‹ã§åˆ©ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ãªå…¬è¡†é€ä¿¡ã«ã¤ã„ã¦ã€
å€‹åˆ¥ã«è‘—ä½œæ¨©è€…ç­‰ã®è¨±è«¾ã‚’å¾—ã‚‹ã“ã¨ãªãè¡Œã†ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
'''

# Initial settings and website and file paths
sys.setrecursionlimit(50)
locale.setlocale(locale.LC_CTYPE, "Japanese_Japan.932")
PATH = 'https://www3.nhk.or.jp/news/easy/'
NEWS_ARTICLE_URL_IDENTIFIER = 'k1001'
NEWS_ARTICLE_TXT_LOCATION = r'./news_article.txt'
PRONOUN_QUIZ_LOCATION = r'./pronunciation_quiz.txt'
DEF_QUIZ_LOCATION = r'./definition_quiz.txt'
PAST_QUIZ_DATA_LOCATION = r'./past_quiz_data.txt'


def get_news_url() -> str:
    """Retrieve up-to-date news url links"""
    opt = webdriver.ChromeOptions()
    opt.add_argument('headless')
    opt.add_experimental_option('excludeSwitches', ['enable-logging'])
    driver = webdriver.Chrome(options=opt)
    driver.get(PATH)

    links = driver.find_elements(By.XPATH, "//a[@href]")
    news_links = [link.get_attribute("href")
                  for link in links if NEWS_ARTICLE_URL_IDENTIFIER in link.get_attribute("href")]
    news_current = list(set(news_links[1:9]))

    try:
        new_url = random.choice(news_current)
    except IndexError:
        return get_news_url()
    return new_url


def write_text_data(content, action='a', location=NEWS_ARTICLE_TXT_LOCATION, encoder='utf-8') -> None:
    """Write text (BeautifulSoup | NavigableString) content to a file"""
    if content is not None:
        with open(location, action, encoding=encoder) as file:
            file.write(content.text.strip() + '\n\n')


def is_hiragana_char(character: str) -> bool:
    """Check if a character is hiragana or not"""
    return u'\u3040' <= character <= u'\u309F'


def today_date() -> Tuple:
    """Return today's date in Japanese"""
    now = datetime.datetime.now()
    return now, now.strftime(f'%Yå¹´%mæœˆ%dæ—¥ {get_day_of_week_jp(now)} %Hæ™‚%Måˆ†')


def get_day_of_week_jp(date) -> List:
    """Return day of the week in Japanese"""
    week_list = ['æœˆæ›œæ—¥', 'ç«æ›œæ—¥', 'æ°´æ›œæ—¥', 'æœ¨æ›œæ—¥', 'é‡‘æ›œæ—¥', 'åœŸæ›œæ—¥', 'æ—¥æ›œæ—¥']
    return week_list[date.weekday()]


def generate_pronunciation_quiz(url: str, word_dict: Dict[str, str], questions=4) -> None:
    """Generate a pronunciation test for students"""
    now, today = today_date()

    # randomly remove questions until the number of questions reach a desired value
    while len(word_dict) > questions:
        word_dict.pop(random.choice(list(word_dict.keys())))

    # write the test to a file
    with open(PRONOUN_QUIZ_LOCATION, 'w', encoding='utf-8') as f:
        f.write(f'ã€èªå½™åŠ›ã‚¯ã‚¤ã‚ºã€‘{today}\n\n')
        f.write(f'ä»Šæ—¥èª­ã‚“ã NHK EASYãƒ‹ãƒ¥ãƒ¼ã‚¹ğŸ“°ã‚’å¾©ç¿’ã—ã¦ã€è¾æ›¸ã‚’è¦‹ã›ãšã«ã‚¹ãƒãƒ›ã§å˜èªãƒ»æ¼¢å­—ã®èª­ã¿æ–¹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚\n' +
                f'ã‚«ã‚¿ã‚«ãƒŠã®å ´åˆã¯æ—¥æœ¬èªã‚‚ã—ãã¯è‹±èªã§æ„å‘³ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚({len(word_dict)}ãƒã‚¤ãƒ³ãƒˆ)\n\n')
        f.write(f'{url}\n\n')
        f.write('---\n\n')
        f.write('å­¦ç”Ÿç•ªå·: \n\n')
    with open(PRONOUN_QUIZ_LOCATION, 'a', encoding='utf-8') as f:
        for i, word in enumerate(word_dict.keys(), start=1):
            letter = string.ascii_uppercase[i-1]
            f.write(f'{letter}. {word}: \n')


def generate_definition_quiz(article, word_dict: Dict[str, str], word_list: List) -> None:
    """Generate a definition test for students"""
    now, today = today_date()

    # randomly remove questions until the number of questions reach a desired value
    new_word_list_header = []
    new_word_list = []
    for key in word_dict.keys():
        for definition in word_list:
            if key == definition.split('ï¼š', 1)[0]:
                new_word_list_header.append(definition.split('ï¼š', 1)[0])
                new_word_list.append(definition.split('ï¼š', 1)[1])

    # Shuffle the order of the questions and print the answer key
    new_word_list = [
        f"{item}{string.ascii_uppercase[i]}" for i, item in enumerate(new_word_list)]
    random.shuffle(new_word_list)
    print(f'\nå˜èªæ„å‘³ã‚¯ã‚¤ã‚ºè§£ç­”ï¼š{"".join([item[-1] for item in new_word_list])}')
    new_word_list = [
        f"{item[:-1]} {string.ascii_uppercase[i]}" for i, item in enumerate(new_word_list)]

    # write the test to a file
    with open(DEF_QUIZ_LOCATION, 'w', encoding='utf-8') as f:
        f.write(f'ã€å˜èªæ„å‘³ã‚¯ã‚¤ã‚ºã€‘{today}\n\n')
        f.write(
            f'ä»Šæ—¥ã®NHK EASYãƒ‹ãƒ¥ãƒ¼ã‚¹ğŸ“°ã§ã™ã€‚(1)ã‹ã‚‰æ­£ã—ã„å˜èªã®æ„å‘³ã‚’é †ç•ªã«ä¸¦ã¹ã¦ãã ã•ã„ã€‚({len(new_word_list)}ãƒã‚¤ãƒ³ãƒˆ)\n\n')

    with open(DEF_QUIZ_LOCATION, 'a', encoding='utf-8') as f:
        for paragraph in article:
            f.write(paragraph.text.strip() + '\n\n')

        f.write('---\n\n')

        for i, word in enumerate(new_word_list_header, start=1):
            f.write(f'({i}) {word} ')

        f.write('\n\n')

        for i, word in enumerate(new_word_list, start=1):
            letter = string.ascii_uppercase[i-1]
            f.write(f'{letter}. {word.split(" ")[1]}\n\n')

        f.write('ã€è¿”ä¿¡ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘(è‹±èªã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã¨æ•°å­—ã®ã¿):\n')
        f.write('å­¦ç”Ÿç•ªå·: A10001\n')
        f.write('è§£ç­”: ABCDE')


def save_quiz_vocab(news_url: str) -> None:
    """Save pushed quiz vocabularies and news url to a file"""
    now, today = today_date()
    with open('news_article.txt', 'r', encoding='utf-8') as f:
        content = f.read()
        parts = content.split('---')
        vocab = parts[1].strip()
        vocab_def = parts[2].strip()
    with open(PAST_QUIZ_DATA_LOCATION, 'a+', encoding='utf-8') as f:
        f.write(f'{today}\n{news_url}\n\n{vocab}\n\n{vocab_def}\n\n')
        f.write('---\n\n')


def push_quiz(test_type: str) -> None:
    """Send message via LINE API to students"""
    with open(test_type, 'r', encoding='utf-8') as f:
        content = f.read()
        parts = content.split('---')
        instruction = parts[0].strip()
        questions = parts[1].strip()

        send_message('text', instruction)
        send_message('text', questions)


def main(test_type: str, push=False, questions=5) -> None:
    """Establish request connection and randomly scrap a Japanese news article's content and vocabularies"""
    # Get and encode a random news url; parsing the HTML content
    url = get_news_url()
    response = requests.get(url)
    response.encoding = response.apparent_encoding

    if response.status_code == 200:
        # HTTP status OK
        html_content = response.text
    else:
        print('Request failed. Check your Internet connection.')
        sys.exit()

    soup = BeautifulSoup(html_content, 'html.parser')

    # Article url (ã‚¢ãƒ‰ãƒ¬ã‚¹)
    with open(NEWS_ARTICLE_TXT_LOCATION, 'w') as f:
        f.write(f'{url}\n\n')

    # Article publishing date (æ²è¼‰æ—¥)
    date = soup.find('p', class_='article-main__date')
    write_text_data(date)

    # Article title (ã‚¿ã‚¤ãƒˆãƒ«)
    title = soup.find('h1', class_='article-main__title')
    write_text_data(title)

    # Article content (å†…å®¹)
    article = soup.find_all('div', class_='article-main__body article-body')
    for paragraph in article:
        write_text_data(paragraph)

    # Important vocabularies (èªå½™)
    vocabulary_list = soup.find_all('a', class_='dicWin')
    vocabulary_dict = {}
    furigana = ''

    for vocabulary in vocabulary_list:
        word = vocabulary.text
        rt = vocabulary.find_all('rt')

        if rt:
            for i, _ in enumerate(rt):
                curr = rt[i].text
                if i > 0:
                    furigana += ' ' + curr
                else:
                    furigana = curr

        else:
            # If keys are ã‚«ã‚¿ã‚«ãƒŠ, give them empty string as values instead
            furigana = ''

        vocabulary_dict[word] = furigana

    # Reformat word: è©±ã—åˆã†: ã¯ãª ã‚ -> è©±(ã¯ãª)ã—åˆ(ã‚)ã†
    formatted_word_list = []

    for key, value in vocabulary_dict.items():
        word = ''
        combine = False
        for char in key:
            if is_hiragana_char(char):
                combine = True

        if combine:
            kana = deque(value.split(' '))
            for i, char in enumerate(key):
                if not is_hiragana_char(char) and is_hiragana_char(key[i + 1]):
                    try:
                        word += f'{char}({kana[0]})'
                        kana.popleft()
                    except IndexError:
                        word += char
                        word = word.replace(f'({value})', '')
                else:
                    word += char
        if word:
            formatted_word_list.append(word)
        else:
            formatted_word = f'{key}({value})'
            if '()' in formatted_word:
                formatted_word = formatted_word.replace('()', '')
            formatted_word_list.append(formatted_word)

    with open(NEWS_ARTICLE_TXT_LOCATION, 'a', encoding='utf-8') as f:
        f.write('---\n')
        for word in formatted_word_list:
            f.write(f'\n{word}')

    # Generate and push quiz to LINE
    generate_pronunciation_quiz(url, vocabulary_dict, questions=questions)

    # Printing news title, date, and url
    if title and date is not None:
        print(f'{title.text.lstrip().rstrip()} {date.text}')
        print(f'{url}\n')

    definition_list = get_definition_list(url, soup)
    with open(NEWS_ARTICLE_TXT_LOCATION, 'a', encoding='utf-8') as f:
        f.write('\n\n---\n\n')
        for definition in definition_list:
            f.write(f'{definition}\n')

    generate_definition_quiz(article, vocabulary_dict, definition_list)

    if push:
        if test_type == 'def':
            push_quiz(DEF_QUIZ_LOCATION)
            save_quiz_vocab(url)
        elif test_type == 'pronoun':
            push_quiz(PRONOUN_QUIZ_LOCATION)
            save_quiz_vocab(url)


if __name__ == '__main__':
    # Linux
    if sys.platform.startswith('linux'):
        os.system('clear')
    # Windows
    elif sys.platform.startswith('win32'):
        os.system('cls')

    # test_type: 'def' -> å˜èªæ„å‘³ or 'pronoun' -> å˜èªç™ºéŸ³
    main(test_type='def', push=False, questions=5)
