# Standard library imports
from typing import Dict, List
import os
import sys
import random
import string
from collections import deque
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Callable

# Third-party imports
import chardet
import locale
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import WebDriverException
from selenium.webdriver.common.by import By

# Local imports
from send_line_message import send_message
from get_definition import get_definition_list, get_number_of_word, setup_selenium

try:
    from check_sentiment import predict_sentiment_jp, read_news_article
except ImportError:
    pass


"""
NEWS WEB EASY
å¹³æ—¥ã®ã¿ã€4ã¤ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ›´æ–°ã•ã‚Œã‚‹
å‡ºæ‰€ã®æ˜ç¤ºã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å†…å®¹ã‚’è»¢è¼‰â—‹ èªå½™ã¯â—‹ è¾æ›¸â–³Ã—ï¼Ÿ

https://www.nhk.or.jp/nijiriyou/kyouiku.html
è‘—ä½œæ¨©æ³•ã§ã¯ã€å­¦æ ¡ãã®ä»–ã®æ•™è‚²æ©Ÿé–¢ã§ã€æˆæ¥­ã®æ•™æã¨ã—ã¦è¤‡è£½ã‚’èªã‚ã‚‹ä»£ã‚ã‚Šã«ã€ã€Œæˆæ¥­æ‹…å½“è€…ã¾ãŸã¯æˆæ¥­ã‚’å—ã‘ã‚‹è€…ãŒè¤‡è£½ã™ã‚‹ã“ã¨ã€ã‚„ã€Œå‡ºæ‰€ã®æ˜ç¤ºã€ãªã©ã€ã„ãã¤ã‹ã®æ¡ä»¶ã‚’å®ˆã‚‹ã‚ˆã†æ±‚ã‚ã¦ã„ã¾ã™ã€‚
ã¾ãŸã€è‘—ä½œæ¨©æ³•ãŒä¸€éƒ¨æ”¹æ­£ã•ã‚Œã€ï¼’ï¼ï¼’ï¼å¹´ï¼”æœˆï¼’ï¼˜æ—¥ã‹ã‚‰ã€æ¨©åˆ©è€…ã®åˆ©ç›Šã‚’ä¸å½“ã«å®³ã—ãªã„ç¯„å›²ã§ã€å­¦æ ¡ã®å…ˆç”Ÿã‚„ç”Ÿå¾’ãŒã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆãªã©ã‚’é€šã˜ã¦ã€ï¼®ï¼¨ï¼«ã®ç•ªçµ„ã‚’æ•™æã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚

https://sartras.or.jp/seido/
æˆæ¥­ç›®çš„å…¬è¡†é€ä¿¡è£œå„Ÿé‡‘åˆ¶åº¦
å…·ä½“çš„ã«ã¯ã€å­¦æ ¡ç­‰ã®æ•™è‚²æ©Ÿé–¢ã®æˆæ¥­ã§ã€äºˆç¿’ãƒ»å¾©ç¿’ç”¨ã«æ•™å“¡ãŒä»–äººã®è‘—ä½œç‰©ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ•™æã‚’ç”Ÿå¾’ã®ç«¯æœ«ã«é€ä¿¡ã—ãŸã‚Šã€ã‚µãƒ¼ãƒã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚Šã™ã‚‹ã“ã¨ãªã©ã€ICTã®æ´»ç”¨ã«ã‚ˆã‚Šæˆæ¥­ã®éç¨‹ã§åˆ©ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ãªå…¬è¡†é€ä¿¡ã«ã¤ã„ã¦ã€å€‹åˆ¥ã«è‘—ä½œæ¨©è€…ç­‰ã®è¨±è«¾ã‚’å¾—ã‚‹ã“ã¨ãªãè¡Œã†ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
"""

# Initial settings and website and file paths
NEWS_HOMEPAGE_URL = "https://www3.nhk.or.jp/news/easy/"
NEWS_ARTICLE_URL_IDENTIFIER = "k1001"
NEWS_ARTICLE_TXT_LOCATION = r"txt_files/news_article.txt"
PRONOUN_QUIZ_LOCATION = r"txt_files/pronunciation_quiz.txt"
DEF_QUIZ_LOCATION = r"txt_files/definition_quiz.txt"
PAST_QUIZ_DATA_LOCATION = r"txt_files/past_quiz_data.txt"
LOG_LOCATION = r"txt_files/push_log.txt"

# Selenium checking settings constants
MAX_URL_CHECKING_ATTEMPTS = 10
MIN_URL_WORD_COUNT = 3

# Set locale to Japanese
if sys.platform.startswith("win32"):
    locale.setlocale(locale.LC_CTYPE, "Japanese_Japan.932")
else:
    locale.setlocale(locale.LC_TIME, "ja_JP.UTF-8")


def generate_pronunciation_quiz(
    url: str, word_dict: Dict[str, str], questions=4
) -> None:
    """Generate a pronunciation test for students"""
    today = get_today_date_jp()[1]

    # randomly remove questions until the number of questions reach a desired value
    while len(word_dict) > questions:
        word_dict.pop(random.choice(list(word_dict.keys())))

    # write the test to a file
    with open(PRONOUN_QUIZ_LOCATION, "w", encoding="utf-8") as f:
        f.write(f"ã€èªå½™åŠ›ã‚¯ã‚¤ã‚ºã€‘{today}\n\n")
        f.write(
            f"ä»Šæ—¥èª­ã‚“ã NHK EASYãƒ‹ãƒ¥ãƒ¼ã‚¹ğŸ“°ã‚’å¾©ç¿’ã—ã¦ã€è¾æ›¸ã‚’è¦‹ãšã«ã‚¹ãƒãƒ›ã§å˜èªãƒ»æ¼¢å­—ã®èª­ã¿æ–¹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚\n"
            + f"ã‚«ã‚¿ã‚«ãƒŠã®å ´åˆã¯æ—¥æœ¬èªã‚‚ã—ãã¯è‹±èªã§æ„å‘³ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚({len(word_dict)}ãƒã‚¤ãƒ³ãƒˆ)\n\n"
        )
        f.write(f"{url}\n\n")
        f.write("---\n\n")
        f.write("å­¦ç”Ÿç•ªå·: \n\n")
        for i, word in enumerate(word_dict.keys(), start=1):
            letter = string.ascii_uppercase[i - 1]
            f.write(f"{letter}. {word}: \n")


def generate_definition_quiz(
    article, word_dict: Dict[str, str], word_list: List
) -> str:
    """Generate a definition test for students and return the answer key"""
    today = get_today_date_jp()[1]

    # Extract and process questions from the word dictionary
    new_word_list_header = []
    new_word_list = []
    for key in word_dict.keys():
        for definition in word_list:
            if key == definition.split("ï¼š", 1)[0]:
                new_word_list_header.append(definition.split("ï¼š", 1)[0])
                new_word_list.append(definition.split("ï¼š", 1)[1])

    # Shuffle the order of the questions and print the answer key
    new_word_list_header = [
        f"{item}{string.ascii_uppercase[i]}"
        for i, item in enumerate(new_word_list_header)
    ]
    random.shuffle(new_word_list_header)
    answer = "".join([item[-1] for item in new_word_list_header])
    print(f"\nå˜èªæ„å‘³ã‚¯ã‚¤ã‚ºè§£ç­”ï¼š{answer}")
    new_word_list_header = [
        f"{item[:-1]} {string.ascii_uppercase[i]}"
        for i, item in enumerate(new_word_list_header)
    ]

    # write the test to a file
    with open(DEF_QUIZ_LOCATION, "w", encoding="utf-8") as f:
        f.write(f"ã€å˜èªæ„å‘³ã‚¯ã‚¤ã‚ºã€‘{today}\n\n")
        f.write(
            f"ä»Šæ—¥ã®NHK EASYãƒ‹ãƒ¥ãƒ¼ã‚¹ğŸ“°ã§ã™ã€‚(1) ã‹ã‚‰æ­£ã—ã„å˜èªã®æ„å‘³ã‚’é †ç•ªã«ä¸¦ã¹ã¦ãã ã•ã„ã€‚({len(new_word_list)}ãƒã‚¤ãƒ³ãƒˆ)\n\n"
        )

        # write the article to a file
        for paragraph in article:
            for p_tag in paragraph.find_all("p"):
                f.write(p_tag.text.strip() + "\n\n")

        f.write("---\n\n")

        for i, word in enumerate(new_word_list_header, start=1):
            f.write(f'({i}) {word.split(" ")[0]} ')

        f.write("\n\n")

        for i, word in enumerate(new_word_list, start=1):
            letter = string.ascii_uppercase[i - 1]
            f.write(f"{letter}. {word}\n\n")

        # write sample answer format
        f.write("ã€è¿”ä¿¡ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘(è‹±èªã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã¨æ•°å­—ã®ã¿):\n")
        f.write("å­¦ç”Ÿç•ªå·: A10001\n")
        f.write("è§£ç­”: ABCDE")

        return answer


def get_news_url(driver: webdriver.Chrome) -> str:
    """Retrieve up-to-date news url links"""
    for _ in range(MAX_URL_CHECKING_ATTEMPTS):
        try:
            driver.get(NEWS_HOMEPAGE_URL)
        except WebDriverException:
            raise ConnectionError("ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã®æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        links = driver.find_elements(By.XPATH, "//a[@href]")
        news_links = [
            link.get_attribute("href")
            for link in links
            if NEWS_ARTICLE_URL_IDENTIFIER in link.get_attribute("href")
        ]
        news_current = list(set(news_links[1:9]))

        # Remove links with less than min_word_count
        news_current = [
            link
            for link in news_current
            if get_number_of_word(link)[0] >= MIN_URL_WORD_COUNT
        ]

        # If links are found, return a random link
        if news_current:
            new_url = random.choice(news_current)
            return new_url

    # If no links are found after max_attempts, return None
    error_message = (
        f"{MAX_URL_CHECKING_ATTEMPTS}å›ã®è©¦è¡Œå¾Œã€{MIN_URL_WORD_COUNT}èªä»¥ä¸Šã®ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    )
    with open(LOG_LOCATION, "w", encoding="utf-8") as file:
        file.write(f"{get_today_date_jp()[1]}\n")
        file.write(f"{error_message}")
    raise RuntimeError(error_message)


def write_text_data(content, action="a", location=NEWS_ARTICLE_TXT_LOCATION, encoder="utf-8") -> None:
    """Write text (BeautifulSoup | NavigableString) content to a file"""
    if content is not None:
        with open(location, action, encoding=encoder) as file:
            for p_tag in content.find_all("p"):
                for line in p_tag.get_text().splitlines():
                    stripped_line = line.strip()
                    if stripped_line:
                        file.write(stripped_line + '\n\n')


def is_hiragana_char(character: str) -> bool:
    """Check if a character is hiragana or not"""
    return "\u3040" <= character <= "\u309F"


def get_today_date_jp() -> Tuple:
    """Return today's date in both datetime and Japanese format"""
    now = datetime.now()
    return now, now.strftime(f"%Yå¹´%mæœˆ%dæ—¥ {get_day_of_week_jp(now)} %Hæ™‚%Måˆ†")


def get_day_of_week_jp(date) -> List:
    """Return day of the week in Japanese"""
    week_list = ["æœˆæ›œæ—¥", "ç«æ›œæ—¥", "æ°´æ›œæ—¥", "æœ¨æ›œæ—¥", "é‡‘æ›œæ—¥", "åœŸæ›œæ—¥", "æ—¥æ›œæ—¥"]
    return week_list[date.weekday()]


def save_quiz_vocab(news_url: str) -> None:
    """Save pushed quiz vocabularies and news url to a file"""
    today = get_today_date_jp()[1]
    with open(NEWS_ARTICLE_TXT_LOCATION, "r", encoding="utf-8") as f:
        content = f.read()
        parts = content.split("---")
        vocab = parts[1].strip()
        vocab_def = parts[2].strip()
    with open(PAST_QUIZ_DATA_LOCATION, "a+", encoding="utf-8") as f:
        f.write(f"{today}\n{news_url}\n{vocab}\n\n{vocab_def}\n\n")
        f.write("---\n\n")


def push_quiz(test_type: str, broadcasting=False) -> None:
    """Send message via LINE API to students"""
    with open(test_type, "r", encoding="utf-8") as f:
        content = f.read()
        parts = content.split("---")
        instruction = parts[0].strip()
        questions = parts[1].strip()

    send_message("text", instruction, broadcasting=broadcasting)
    send_message("text", questions, broadcasting=broadcasting)


def log_sentiment_score() -> Dict[str, str]:
    """Get the sentiment score of the text."""
    article = read_news_article()
    score_dict = predict_sentiment_jp(article)
    return score_dict


def clear_terminal() -> None:
    os.system("cls" if os.name == "nt" else "clear")


def main(
    quiz_type: str,
    push=False,
    broadcasting=False,
    questions=5,
    progress_callback: Optional[Callable] = None,
) -> None:
    """Establish request connection and randomly scrap a Japanese news article's content and vocabularies"""
    # Get and encode a random news url; parsing the HTML content
    driver = setup_selenium()
    url = get_news_url(driver)

    # Get the article vocabularies and definitions
    definition_list = get_definition_list(driver, url, progress_callback)
    driver.close()

    # Establish a request connection to the url
    response = requests.get(url)
    encoding = chardet.detect(response.content)["encoding"]
    response.encoding = encoding
    if response.status_code == 200:
        # HTTP status OK
        html_content = response.text
    else:
        sys.exit("Request failed. Check your Internet connection.")
    soup = BeautifulSoup(html_content, "html.parser")

    # Article url (ã‚¢ãƒ‰ãƒ¬ã‚¹)
    with open(NEWS_ARTICLE_TXT_LOCATION, "w") as f:
        f.write(f"{url}\n\n")

    # Article publishing date (æ²è¼‰æ—¥)
    date = soup.find("p", class_="article-main__date")
    write_text_data(date)

    # Article title (ã‚¿ã‚¤ãƒˆãƒ«)
    title = soup.find("h1", class_="article-main__title")
    write_text_data(title)

    # Article content (å†…å®¹)
    article = soup.find_all("div", class_="article-main__body article-body")
    for paragraph in article:
        write_text_data(paragraph)

    # Important vocabularies (èªå½™)
    vocabulary_list = soup.find_all("a", class_="dicWin")
    vocabulary_dict = {}
    furigana = ""

    # Create a dictionary of vocabulary: furigana
    for vocabulary in vocabulary_list:
        word = vocabulary.text
        rt = vocabulary.find_all("rt")

        if rt:
            for i, _ in enumerate(rt):
                curr = rt[i].text
                if i > 0:
                    furigana += " " + curr
                else:
                    furigana = curr

        else:
            # If keys are ã‚«ã‚¿ã‚«ãƒŠ, give them empty string as values instead
            furigana = ""

        vocabulary_dict[word] = furigana

    # Reformat word: è©±ã—åˆã†: ã¯ãª ã‚ -> è©±(ã¯ãª)ã—åˆ(ã‚)ã†
    formatted_word_list = []
    for key, value in vocabulary_dict.items():
        word = ""
        combine = False
        for char in key:
            if is_hiragana_char(char):
                combine = True

        # If the word is a combination of hiragana, combine the furigana
        if combine:
            key += " "
            kana = deque(value.split(" "))
            for i, char in enumerate(key):
                if (
                    not is_hiragana_char(char)
                    and (i + 1 < len(key))
                    and (is_hiragana_char(key[i + 1]) or " ")
                ):
                    try:
                        word += f"{char}({kana[0]})"
                        kana.popleft()
                    except IndexError:
                        word += char
                        word = word.replace(f"({value})", "")
                else:
                    word += char
        if word:
            word = word.replace(" ", "")
            formatted_word_list.append(word)
        else:
            formatted_word = f"{key}({value})"
            if "()" in formatted_word:
                formatted_word = formatted_word.replace("()", "")
            formatted_word_list.append(formatted_word)

    # Write formatted vocabularies to a file
    with open(NEWS_ARTICLE_TXT_LOCATION, "a", encoding="utf-8") as f:
        f.write("---\n")
        for word in formatted_word_list:
            f.write(f"\n{word}")

    # Printing news title, date, and url
    if title and date:
        print(f"\n{title.text.strip()} {date.text}")
        print(f"{url}\n")

    # Modify the definition list to include the original word; get current progress

    definition_list_original_word = []
    for key, meaning in zip(vocabulary_dict.keys(), definition_list):
        try:
            definition_list_original_word.append(
                key + "ï¼š" + meaning.split("ï¼š", 1)[1])
        except IndexError:
            print(
                f"\nWARNING: definition_list is missing a or some element(s). This is a rare case due to incomplete scrapping of selenium. Consider re-running the program or change the scale for document.body.style.transform in get_definition.py."
            )
            pass

    # Save definitions to a news_article.txt file
    with open(NEWS_ARTICLE_TXT_LOCATION, "a", encoding="utf-8") as f:
        f.write("\n\n---\n\n")
        for definition in definition_list_original_word:
            f.write(f"{definition}\n")

    # Generate the pronunciation quiz
    generate_pronunciation_quiz(url, vocabulary_dict, questions=questions)

    # Get the answer to the definition quiz and generate the definition quiz
    def_answer = generate_definition_quiz(
        article, vocabulary_dict, definition_list_original_word
    )

    # Save quiz sent time and news url to a log file
    with open(LOG_LOCATION, "w", encoding="utf-8") as f:
        now = get_today_date_jp()[0]
        now = now.strftime(f"%Y-%m-%d %H:%M:%S")
        f.write(f"{now}\n{url}\nå˜èªæ„å‘³ã‚¯ã‚¤ã‚ºè§£ç­”ï¼š{def_answer}\n")
        try:
            scores_dict = log_sentiment_score()
            for sentiment, score in scores_dict.items():
                f.write(f"{sentiment}: {score}\n")
        except NameError:
            print("Sentiment analysis module failed. Skipping sentiment analysis.")

    # Push quiz to LINE if push is True
    if push:
        if quiz_type == "å˜èªæ„å‘³ã‚¯ã‚¤ã‚º":
            push_quiz(DEF_QUIZ_LOCATION, broadcasting=broadcasting)
            save_quiz_vocab(url)
        elif quiz_type == "èª­ã¿æ–¹ã‚¯ã‚¤ã‚º":
            push_quiz(PRONOUN_QUIZ_LOCATION, broadcasting=broadcasting)
            save_quiz_vocab(url)

        # Save quiz sent time and news url to a log file
        with open(LOG_LOCATION, "a", encoding="utf-8") as f:
            f.write("é€ä¿¡æ¸ˆã¿\n")


if __name__ == "__main__":
    # Clearing the terminal
    clear_terminal()

    # quiz_type: 'å˜èªæ„å‘³ã‚¯ã‚¤ã‚º' or 'èª­ã¿æ–¹ã‚¯ã‚¤ã‚º'
    main(quiz_type="å˜èªæ„å‘³ã‚¯ã‚¤ã‚º", push=False, broadcasting=False, questions=5)
